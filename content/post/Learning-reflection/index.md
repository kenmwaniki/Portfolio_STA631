---
date: "2025-04-15T13:09:13-06:00"
title: Reflection on Learning and Participation
---


# Course Learning Objective 1: Describe probability as a foundation of statistical modelling including inference and maximum likelihood estimation

In several assignments — namely Homework 1 (Simple Linear Regression), Homework 2 (Multiple Linear Regression), Homework 4 (Multinomial Logistic Regression), and the GLM Cross-Validation Project — I showcased how these fundamental ideas of probability are applied to compile, validate and understand statistics models. My theoretical knowledge in relation to concepts, such as confidence intervals, p-values, and maximum likelihood estimation (MLE), is furthermore observed in my reflection one.

In Homework 1, I looked at how probability enables us to make conclusions about how variables are related, e.g. how horsepower influences mpg. i was interpreting the statistical significance of the regression coefficients in terms of p-values and confidence intervals—all based on probability theory. remember that rounexpectedly high p-value for horsepower, that (\< 0.0001) suggest that the relationship with mpg is significant and negative. i also talked about the significance of the normal distribution assumption of residuals, because it underpins the validity of the inferential statistics i am using. Homework 2 strengthened this bond by expanding to multiple predictors, embedding my foundation of inference to larger models.

Homework 4 applied a multinomial logistic regression model, which is estimated with maximum likelihood estimation (MLE). The equations and conceptual framework i outlined in my write-up show that i understand how MLE works by maximizing the probability of observing the given data under the model.Using demographic variables such as age, education level, and income level, I modeled the likelihood for each person to fall into one of the voter behavior categories. This involved working with the odds ratio — calculating probability ratios (log-odds) and interpreting them using exponentiated coefficients (odds ratios) — directly making use of probabilistic modelling. my model took the “rarely/never” group as a baseline and calculated the probability of being in the other categories, relative to that baseline—which is an inherently probabilistic approach (i.e., one that leverages distributions, such as the multinomial).

Additionally, I used cross-validation to estimate how well models generalize to new data on my GLM Project and homework 5. Here, i used probabilistic thinking to help me read residuals, variance and metrics like RMSE. i’ve used fold-wise sampling to build empirical distributions of model performance which reflects my increasing comfort with using probability to verbose uncertainty and variability in predictions.

# Course Learning Objective 2: Apply the appropriate generalized linear model (GLM) for a specific data context

This learning goal is strongly demonstrated in my Homework 3 (Categorical Predictors), Homework 4 (Multinomial Logistic Regression), and my GLM Cross-Validation Project. Both of these assignments asked you to evaluate the structure of my data and then apply an appropriate generalized linear model (GLM), particularly when you could not rely on a categorical response or some form of nonnormal error structure. my selection of models, rationalization of variable roles, and the structure of the modeling workflow and validation process using tidymodels showcase my strength in this area.

However, in Homework 3, I started to look at categorical predictors and their interaction with the response variable. I properly coded factor variables, you used logistic regression as your GLM, and you interpreted the coefficients in odds ratio terms. i did dummy variable encoding with care, including picking a meaningful reference category and clearly showing the impact vectors representing categorical variables have offer destruction. With this foundational work, I was set up for Homework 4, where I extended the analysis to a multinomial logistic regression model. There, my outcome variable consisted of three unordered categories (voter behavior: always, sporadic, rarely/never), and I assumed correctly that a standard logistic regression would not be suitable. I used the multinomial model with the multinom_reg() function from tidymodels, set the baseline category correctly and interpreted the model output with exponentiated coefficients (odds ratios). Clearly, these choices established my increasing confidence in choosing and justifying a GLM for a form of data.

In the GLM Cross-Validation Project, I implemented logistic regression to predict self-reported academic performance (high grades vs. low grades) based on a set of behavioral and demographic predictors. This was yet another beautiful case of model choice based on data type: My outcome was binary, thus the appropriate GLM was logistic regression. Most importantly, I performed resampling (v-fold cross-validation) and validated my model and justified my model selection not only based on type of outcome but also type of predictive performance. It invoked steps for encoding of factors, scaling of numeric predictors, and tuning of the model itself, a reasonable (if not dependent) understanding of the role of preprocessing in a GLM setting. Overall, these assignments demonstrate that I can identify a problem, decide if a GLM is appropriate, and apply it with the correct data preprocessing and validation.

## Course Learning Objective 3: Demonstrate model selection given a set of candidate models

This learning objective was demonstrated in Homework 5, Homework 6, and my GLM Cross-Validation Project, which allowed for many predictors but required judgement on which to include in the final model. Every assignment forced me to weigh models against each other and make careful decisions informed both by statistical performance and theoretical reasoning.

In Homework 5, I began with fitting simple linear models for single predictor variables like horsepower, weight, and displacement to predict car acceleration. Next, I built multiple regression models to combine predictors. Both forward and backward stepwise selection were performed to reduce overfitting and limit unnecessary predictors. I examined performance metrics such as the adjusted R-squared and RMSE, and I also examined residual plots to confirm that the assumptions of linear regression were satisfied. This exercise taught me that adding more variables isn’t necessarily better, and that model selection is all about weighing accuracy against simplicity and interpretability.

In Homework 6, I experimented with polynomial regression by tuning the degree of each of the variables (i.e horsepower, displacement) using the step_poly() function. I wanted to be able to model nonlinear relationships but at the same time control for the complexity of the model. In addition, I leveraged with parallel processing to speed up the tuning process, this was very useful to try a lot of polynomial combinations. Models were evaluated based on cross-validated RMSE and a final model was selected from the best performing ones. This assignment was less vague than others, and I was more confident about knowing how to interpret the effect of each modeling choice and justify why I prefer one model to the other.

When I predict student academic performance in my GLM Cross-Validation Project, I used model selection techniques again. The final model was identified by comparing various sets of predictors through v-fold cross-validation and choosing the model with the optimal trade-off between accuracy and interpretability. I also made more contextual decisions — for instance, deciding to include sleep patterns and other behavioral variables that would logically correlate with academic performance. These experiences taught me to view model selection as both an art and science that relied on data and domain experience to make informed decisions.

## Course Learning Objective 4.  Express the results of statistical models to a general audience

This semester, I worked hard on my ability to explain models with simple, understandable language. I practiced this with Homework 3, when I interpreted categorical predictors, and in Project 1, when I summarized logistic regression model output predicting whether students passed. Each of the two positions also provided opportunities to write insights for a non-technical audience and navigate how best to convey complex results.

In Homework 3, I predicted a binary outcome using categorical variables and had to interpret the effect of different categories with respect to a reference group. I also realized that it was not enough to just report the coefficient or odds ratio. So where I might say, “Group A has a coefficient of 1.5,” I said, “Group A is more likely than the reference group to show the outcome.” I also used odds ratios to make the results more intuitive and practiced writing out what those odds mean in real-world terms. This helped me to realise the true importance of meaning in numbers.

I built a logistic regression model to do this in Project 1, and then I reported my findings with visualizations and plain language summaries. I used ggplot2 to generate bar plots and predictions charts and described model accuracy and confusion matrix results about how well the model can classify students. With the broom package I also extracted tidy summaries of my model, this was a much more manageable piece of information to weed through to find coefficients that were significant. Rather than attending to every detail, I selected the most relevant results and explained them in a way that a nonstatistician — a school administrator or policymaker — might understand. The experience taught me that good communication is as important as good modeling, especially when dealing with real-world data and data that affect people’s decisions.

## Course Learning Objective 5.  Use programming software to fit and assess statistical models

During the semester, I have used R and tidymodels to fit, tweet and inform statistical models every week. This clearly appeared in Homework 2, Homework 6, and the GLM Project.

Homework 2: I worked through multiple linear regression on the Auto dataset in homework 2 and used recipes to normalize and prep the data. I have divided the data into training and test and then to build a modeling pipeline using workflow(). I applied functions such as metrics() and collect_predictions() for model performance evaluation and residuals visualization. This assignment was my first time implementing a whole modeling pipeline from the import of the data, all the way to the final validation, within tentacle principles.

During the GLM Project, I built upon those skills with the use of logistic regression to predict student performance. I trained the model using vfold_cv, and interpreted accuracy and AUC results. I also cleaned data and reapplied factor levels to get a properly working variables. And, in Homework 6, I used step_poly() to tune hyperparameters efficiently. And all these experiences built my credentials for the ability to not only write R code but also to administer an entire pipeline from raw data to a final model evaluation.
