---
date: "2025-04-15T13:09:13-06:00"
title: Reflection on Learning and Participation
---
Learning Objectives
## Course Objective 1: Describe probability as a foundation of statistical modeling, including inference and maximum likelihood estimation
In the multinomial logistic regression model I did on the PFI dataset, I used maximum likelihood estimation via the nnet::multinom function and queried the model performance using cross validation metrics.

The multinomial regression section demonstrates the key role played by probability theory in building a model by maximizing the likelihood of observing the data given the model parameters. E.g. The model's set of coefficients corresponds to log-odds for each of the grade categories, and thus the probability of each of these categories is computed using softmax function that is based on probability theory. AIC as a model selection tool is consistent, given that it is based on likelihood-based criteria that capacity the most parsimonious model, the least number of parameters that gives optimal fit.

I also used Cohen’s Kappa to assess agreement above chance, further stressing how probabilistic reasoning aids interpretation. This adjusts for the chance of random classification, highlighting the importance of probability in model evaluation. Additionally, while training classification models, I used stratified sampling to maintain class proportions, reflecting an understanding of underlying probabilities in data partitioning. These instructions assisted me in ingraining in my mind how statistical models are constructed on and evaluated through probability.
## Course Objective 2: Apply the appropriate generalized linear model for a specific data context
I used cross-validation to check that the error of self-reported grades for the most common response variables was less than that predicted using a multiple linear GLM on one response variable.
The diamond dataset model used linear regression and interaction between a continuous (price) and categorical (high clarity) variable because the outcome was continuous. I verified model assumptions including linearity, independence, and homoscedasticity with respect to residuals (residual plots), and checked assumptions with respect to error terms (Q-Q plots). It was this model which taught me how to incorporate categorical predictors via dummy encoding while obtaining interpretability at the same time.
For the PFI dataset, as the outcome variable (SEGRADES) contained five categories without an ordinal level order, I performed multinomial logistic regression. It was a more sophisticated modelling problem as careful variable selection and the interpretation of the results in terms of log-odds ratios were needed. As another aspect I did classification with LDA and variable reduction with PCA. Based on outcome type and predictor distribution, with appropriate transformations and/or encoding, I learned how to pick the right models.

## Learning Objective 3: Demonstrate model selection given a set of candidate models
I utilized backward stepwise selection by AIC on the multinomial logistic regression model to pare down the overall predictor set from 28 to 13 significant features I would have preferred to include in the final analysis.
I applied the stepAIC() function to iteratively drop variables that did not contribute significantly to the model fit, optimizing the AIC. This has not only decreased overfitting but maximized generalizability. As a result, the model had a much lower AIC and better cross-validation accuracy.
Moreover, during linear regression modeling, I used VIF analysis to detect and exclude multicollinearity variables (e.x. the dimensions x, y, and z of the diamonds dataset) I performed ANOVA tests comparing categorical predictors for clarity, cut, and color as well. These model selection instruments allowed me to test which predictors were significant and which could be excluded, allowing the model to run both with greater interpretability and predictive accuracy.
## Learning Objective 4: Express the results of statistical models to a general audience
I utilized barplots, boxplots and heatmaps throughout the project to visualize relationships between the various predictors and outcomes I would use in my modeling, including a confusion matrix heatmap for classification models.
I paired numerical summaries with interpretable plots to describe findings. For example, the bar plot of the homework help vs SEGRADES had the normalizing included and made it easy for a general audience to follow that higher levels of support lead to a better self-reported grade. Similarly, the carat vs price scatter plot gave us visual insight into a clear trend without requiring regression equations.
In my LDA and multinomial regression with regards to accuracy a heatmap of the confusion matrix shows where we were strong and where we were weak, and the color gradient gives us an intuitive sense of accuracy. I used tibble-type output tables of coefficients to show variable importance and give more emphasis to the direction and size of effects. These tools simplified the processes where stakeholders without much statistical training could interpret how models arrived at particular predictions, and why. Along with the images, I also wrote descriptions of model metrics at the end of the document. For example, I explained that RMSE was the mean distance (or deviation) between predicted and actual carat values from the diamond dataset and that MAE reflected the average absolute error. For the multinomial classification model, I explained, the accuracy of 64.5% showed us how often it got the answer right, and Cohen’s Kappa allowed us to compare how much better the model performed compared to random guessing. These simple explanations put the numbers in context and made the statistical results more understandable to the layman. I also offered a written description of how LASSO variable selection beasts relate to the lived experience of students and family dynamics. To illustrate, I explained that SEENJOY is an indicator of school enjoyment, which is associated with motivation and engagement, whereas SEGRADEQ is a proxy for self-confidence. I also had to translate things like EINTNET (internet access) or FHWKHRS (homework hours) or TTLHHINC (income) into concrete impacts on students’ learning environments. I also described how PARINV—along with variables for FSVOL, FSCOUNSLR, and FHHELP—represents proxy underpinnings of support systems affecting achievement. This narrative interpretation linked parameters of the model with humanizing context, thus giving a more well-rounded insight that an average audience member could take away and relate to.
##Learning Objective 5: Use programming software to fit and assess statistical models
My final portfolio contains the tidymodels framework in R (parsnip, workflows, yardstick) used for every step of model training and validation.
Throughout the semester, I used R programming to implement and evaluate models. Next, I did cross-validation using vfold_cv(), hyperparameter tuning, and tested some metrics like accuracy, Kappa, RMSE and MAE. For instance, the diamond dataset — I had out RMSE and MAE to check the prediction accuracy of a carat-size model.
The classification results were evaluated with yardstick::metrics() for the accuracy and Kappa, conf_mat() and autoplot() for plotting a confusion matrix. I also look for penalized models like LASSO with using glmnet and tune_grid() to help hyperparameter tuning. These practices strengthened how to evaluate model robustness, prevent overfit and recognize performance trade-offs. Having multiple packages to compose together within a single ecosystem allowed me to create modular, reproducible pipelines, which is a hallmark of professional data science work.

